# -*- coding: utf-8 -*-
"""Untitled20_(1)_(2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-PrJkAG9OKLONU8h7bfAKstBYmblJpcI
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from imblearn.over_sampling import SMOTE
import xgboost as xgb
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.model_selection import GridSearchCV


#  Load the data
data = pd.read_csv('/content/diabetes_dataset.csv')
#preprossing the data


sns.set(style="whitegrid")

# Loop through each column in the DataFrame and create a plot
for column in data.columns:
    plt.figure(figsize=(10, 6))

    if data[column].dtype == 'object':
        # For categorical features weuse count plot
        sns.countplot(data[column], palette='Set2')
        plt.title(f'Distribution of {column}')
        plt.xlabel(column)
        plt.ylabel('Count')
    else:
        # For numerical features we use a histogram
        sns.histplot(data[column], bins=30, kde=True, color='blue')
        plt.title(f'Distribution of {column}')
        plt.xlabel(column)
        plt.ylabel('Frequency')

    plt.xticks(rotation=45)  # Rotate x labels if necessary
    plt.tight_layout()
    plt.show()

print(data.isnull().sum())

# basic information about the DataFrame
print("Data Info:")
print(data.info())

# Print a statistical summary of the DataFrame
print("\nData Description:")
print(data.describe())

# Print the shape of the data
print("\nShape of the Data:")
print(data.shape)

missing_smoking_history_count = data['smoking_history'].value_counts().get('No Info', 0)
print("\nCount of 'No Info' in 'smoking_history':")
print(missing_smoking_history_count)

# Remove rows where 'smoking_history' is 'No Info'
data = data[data['smoking_history'] != 'No Info']

# Print the shape of the data after removing rows
print("\nShape of the Data after removing 'No Info' rows:")
print(data.shape)

# Save the modified dataset
data.to_csv('diabetes_dataset_modified.csv', index=False)

# Display the first few rows of the modified DataFrame
print("\nFirst few rows of the modified DataFrame:")
print(data.head())

# Drop unnecessary columns and one-hot encode categorical variables

data = pd.get_dummies(data, columns=['gender','smoking_history'])

correlation_matrix = data.corr()
target_correlation = correlation_matrix['diabetes'].sort_values(ascending=False)
print("Correlation with Target Variable (diabetes):")
print(target_correlation)
#  Visualize the correlation matrix using a heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', square=True, cbar_kws={"shrink": .8})
plt.title('Correlation Matrix Heatmap')
plt.show()

# Separate features and target variable
X = data.drop('diabetes', axis=1)
y = data['diabetes']

# Stratified train-test split
stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_index, test_index in stratified_split.split(X, y):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

# Resampling and scaling
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

scaler = StandardScaler()
X_train_resampled = scaler.fit_transform(X_train_resampled)
X_test = scaler.transform(X_test)

#  Train the model
model = xgb.XGBClassifier(random_state=42)
model.fit(X_train_resampled, y_train_resampled)

# Calculate and print the accuracy of the training data
y_train_pred = model.predict(X_train_resampled)
train_accuracy = accuracy_score(y_train_resampled, y_train_pred)
print(f"Train Accuracy: {train_accuracy}")

# Evaluate on the test set
y_test_pred = model.predict(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
print(f"Test Accuracy: {test_accuracy}")

# Extract precision, recall, and F1 score from the classification report
precision_test = report_test['1']['precision']
recall_test = report_test['1']['recall']
f1_test = report_test['1']['f1-score']
print(classification_report(y_test, y_test_pred))

#  Confusion Matrix for initial model
conf_matrix_before = confusion_matrix(y_test, y_test_pred)
sns.heatmap(conf_matrix_before, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix (Before Tuning)')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# ROC Curve for the initial model
fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:, 1])
roc_auc_before = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='blue', label='ROC curve (area = %0.2f)' % roc_auc_before)
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve (Before Tuning)')
plt.legend(loc='lower right')
plt.show()

# Define parameter grid with a wider range of values
param_grid = {
    'max_depth': [3, 5, 7, 9],          # More options for tree depth
    'min_child_weight': [1, 2, 3, 4],   # Options for minimum child weight
    'subsample': [0.6, 0.7, 0.8, 0.9],   # More options for subsampling
    'colsample_bytree': [0.6, 0.7, 0.8]  # More options for feature subsampling
}

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(
    xgb.XGBClassifier(random_state=42),
    param_grid,
    cv=5,      # Use 5-fold cross-validation
    scoring='f1',  # Optimize for F1 score
    n_jobs=-1
)

# Fit grid search to the resampled training data
grid_search.fit(X_train_resampled, y_train_resampled)

# Best parameters found by grid search
print("Best Parameters:", grid_search.best_params_)
best_model = grid_search.best_estimator_

# Evaluate on the test set
y_pred_tuned = best_model.predict(X_test)
accuracy_tuned = accuracy_score(y_test, y_pred_tuned)
print(f"Tuned Model Accuracy: {accuracy_tuned}")

# Calculate additional metrics for the tuned model
report_tuned = classification_report(y_test, y_pred_tuned, output_dict=True)
precision_tuned = report_tuned['1']['precision']
recall_tuned = report_tuned['1']['recall']
f1_tuned = report_tuned['1']['f1-score']
print(classification_report(y_test, y_pred_tuned))

#  Confusion Matrix for Tuned Model
conf_matrix_tuned = confusion_matrix(y_test, y_pred_tuned)
sns.heatmap(conf_matrix_tuned, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix (After Tuning)')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
# ROC Curve for the Tuned Model
fpr, tpr, thresholds = roc_curve(y_test, best_model.predict_proba(X_test)[:, 1])
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='blue', label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

# Calculate and print the accuracy of the training data after tuning
y_train_pred_tuned = best_model.predict(X_train_resampled)
train_accuracy_tuned = accuracy_score(y_train_resampled, y_train_pred_tuned)
print(f"Training set Accuracy: {train_accuracy_tuned}")

correlation_matrix = data.corr()

# Extract correlation with the target variable 'diabetes'
diabetes_correlation = correlation_matrix['diabetes'].sort_values(ascending=False)

# Print the correlation with the target variable
print("\nCorrelation with Target Variable (diabetes):")
print(diabetes_correlation)

# Plot the correlation matrix as a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', square=True, linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()

# Train KNN model for comparison
knn_model = KNeighborsClassifier()
knn_model.fit(X_train_resampled, y_train_resampled)
y_pred_knn = knn_model.predict(X_test)
accuracy_knn = accuracy_score(y_test, y_pred_knn)

# Train Decision Tree model for comparison
tree_model = DecisionTreeClassifier()
tree_model.fit(X_train_resampled, y_train_resampled)
y_pred_tree = tree_model.predict(X_test)
accuracy_tree = accuracy_score(y_test, y_pred_tree)

# Calculate precision, recall, and F1-score for each model
report_tuned = classification_report(y_test, y_pred_tuned, output_dict=True)
report_knn = classification_report(y_test, y_pred_knn, output_dict=True)
report_tree = classification_report(y_test, y_pred_tree, output_dict=True)

# Extract metrics for class 1
precisions = [
    report_tuned['1']['precision'],
    report_knn['1']['precision'],
    report_tree['1']['precision']
]

recalls = [
    report_tuned['1']['recall'],
    report_knn['1']['recall'],
    report_tree['1']['recall']
]

f1_scores = [
    report_tuned['1']['f1-score'],
    report_knn['1']['f1-score'],
    report_tree['1']['f1-score']
]

# Print accuracy, precision, recall, and F1 scores for class 1
print("\nAlgorithm Comparison for Class 1:")
print(f"Tuned XGBoost Accuracy: {accuracy_tuned:.2f}, Precision: {precisions[0]:.2f}, Recall: {recalls[0]:.2f}, F1 Score: {f1_scores[0]:.2f}")
print(f"KNN Accuracy: {accuracy_knn:.2f}, Precision: {precisions[1]:.2f}, Recall: {recalls[1]:.2f}, F1 Score: {f1_scores[1]:.2f}")
print(f"Decision Tree Accuracy: {accuracy_tree:.2f}, Precision: {precisions[2]:.2f}, Recall: {recalls[2]:.2f}, F1 Score: {f1_scores[2]:.2f}")

# Create a figure for the plots for class 1
models = ['XGBoost', 'KNN', 'Decision Tree']
plt.figure(figsize=(14, 10))

# Accuracy plot for class 1
plt.subplot(2, 2, 1)
plt.bar(models, [accuracy_tuned, accuracy_knn, accuracy_tree], color='blue', alpha=0.6)
plt.ylim(0, 1)
plt.title('Model Accuracies for Class 1')
plt.ylabel('Accuracy')

# Precision plot for class 1
plt.subplot(2, 2, 2)
plt.bar(models, precisions, color='orange', alpha=0.6)
plt.ylim(0, 1)
plt.title('Model Precisions for Class 1')
plt.ylabel('Precision')

# Recall plot for class 1
plt.subplot(2, 2, 3)
plt.bar(models, recalls, color='green', alpha=0.6)
plt.ylim(0, 1)
plt.title('Model Recalls for Class 1')
plt.ylabel('Recall')

# F1 Score plot for class 1
plt.subplot(2, 2, 4)
plt.bar(models, f1_scores, color='purple', alpha=0.6)
plt.ylim(0, 1)
plt.title('Model F1 Scores for Class 1')
plt.ylabel('F1 Score')

# Show metrics plots for class 1
plt.tight_layout()
plt.show()

# Calculate precision, recall, and F1-score for the other label (class 0)
report_tuned_0 = classification_report(y_test, y_pred_tuned, output_dict=True)
report_knn_0 = classification_report(y_test, y_pred_knn, output_dict=True)
report_tree_0 = classification_report(y_test, y_pred_tree, output_dict=True)

# Extract metrics for class 0
precisions_0 = [
    report_tuned_0['0']['precision'],
    report_knn_0['0']['precision'],
    report_tree_0['0']['precision']
]

recalls_0 = [
    report_tuned_0['0']['recall'],
    report_knn_0['0']['recall'],
    report_tree_0['0']['recall']
]

f1_scores_0 = [
    report_tuned_0['0']['f1-score'],
    report_knn_0['0']['f1-score'],
    report_tree_0['0']['f1-score']
]

# Print accuracy, precision, recall, and F1 scores for class 0
print("\nAlgorithm Comparison for Class 0:")
print(f" XGBoost Accuracy: {accuracy_tuned:.2f}, Precision: {precisions_0[0]:.2f}, Recall: {recalls_0[0]:.2f}, F1 Score: {f1_scores_0[0]:.2f}")
print(f"KNN Accuracy: {accuracy_knn:.2f}, Precision: {precisions_0[1]:.2f}, Recall: {recalls_0[1]:.2f}, F1 Score: {f1_scores_0[1]:.2f}")
print(f"Decision Tree Accuracy: {accuracy_tree:.2f}, Precision: {precisions_0[2]:.2f}, Recall: {recalls_0[2]:.2f}, F1 Score: {f1_scores_0[2]:.2f}")

# Create a figure for the plots for class 0
plt.figure(figsize=(14, 10))

# Accuracy plot for class 0
plt.subplot(2, 2, 1)
plt.bar(models, [accuracy_tuned, accuracy_knn, accuracy_tree], color='blue', alpha=0.6)
plt.ylim(0, 1)
plt.title('Model Accuracies for Class 0')
plt.ylabel('Accuracy')

# Precision plot for class 0
plt.subplot(2, 2, 2)
plt.bar(models, precisions_0, color='orange', alpha=0.6)
plt.ylim(0, 1)
plt.title('Model Precisions for Class 0')
plt.ylabel('Precision')

# Recall plot for class 0
plt.subplot(2, 2, 3)
plt.bar(models, recalls_0, color='green', alpha=0.6)
plt.ylim(0, 1)
plt.title('Model Recalls for Class 0')
plt.ylabel('Recall')

# F1 Score plot for class 0
plt.subplot(2, 2, 4)
plt.bar(models, f1_scores_0, color='purple', alpha=0.6)
plt.ylim(0, 1)
plt.title('Model F1 Scores for Class 0')
plt.ylabel('F1 Score')

# Show metrics plots for class 0
plt.tight_layout()
plt.show()

# Confusion Matrices for each model
models = ['XGBoost', 'KNN', 'Decision Tree']
predictions = [y_pred_tuned, y_pred_knn, y_pred_tree]

for i, model_name in enumerate(models):
    conf_matrix = confusion_matrix(y_test, predictions[i])
    plt.figure(figsize=(6, 5))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix for {model_name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

# ROC Curve for all models
plt.figure(figsize=(10, 8))

# ROC for Tuned XGBoost
fpr_tuned, tpr_tuned, _ = roc_curve(y_test, best_model.predict_proba(X_test)[:, 1])
roc_auc_tuned = auc(fpr_tuned, tpr_tuned)
plt.plot(fpr_tuned, tpr_tuned, color='blue', label='Tuned XGBoost (area = %0.2f)' % roc_auc_tuned)

# ROC for KNN
fpr_knn, tpr_knn, _ = roc_curve(y_test, knn_model.predict_proba(X_test)[:, 1])
roc_auc_knn = auc(fpr_knn, tpr_knn)
plt.plot(fpr_knn, tpr_knn, color='orange', label='KNN (area = %0.2f)' % roc_auc_knn)

# ROC for Decision Tree
fpr_tree, tpr_tree, _ = roc_curve(y_test, tree_model.predict_proba(X_test)[:, 1])
roc_auc_tree = auc(fpr_tree, tpr_tree)
plt.plot(fpr_tree, tpr_tree, color='green', label='Decision Tree (area = %0.2f)' % roc_auc_tree)

# Plot formatting
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve Comparison')
plt.legend(loc='lower right')
plt.show()

